# spark

- [åŸºæœ¬æ¦‚å¿µ](#åŸºæœ¬æ¦‚å¿µ)
  - [ç»„ä»¶è¯´æ˜](#ç»„ä»¶è¯´æ˜)
- [RDD](#rdd)
- [Spark SQL](#spark-sql)
  - [Dataset](#dataset)
  - [DataFrame](#dataframe)
- [Spark Streaming](#spark-streaming)
  - [Discretized Streams(DStream)](#discretized-streamsdstream)
  - [Input DStreams and Receivers](#input-dstreams-and-receivers)
    - [Queue of RDDs as a Stream](#queue-of-rdds-as-a-stream)
    - [Transformations on DStream](#transformations-on-dstream)
    - [UpdateStateByKey Operation](#updatestatebykey-operation)
    - [Typical Datasource Imtegration](#typical-datasource-imtegration)
  - [Window Operation](#window-operation)
  - [DataFrame and SQL Operations](#dataframe-and-sql-operations)
  - [Check poing](#check-poing)
    - [When to enable Checkpointing](#when-to-enable-checkpointing)
    - [How to configure Checkpoint](#how-to-configure-checkpoint)
  - [Exactly once](#exactly-once)
  - [MLlib Operations](#mllib-operations)
  - [Monitoring](#monitoring)
- [Structured Streaming](#structured-streaming)
  - [Programming Model](#programming-model)
    - [Basic Concepts](#basic-concepts)
    - [Handling Event-time and late Data](#handling-event-time-and-late-data)
    - [Fault Tolerance](#fault-tolerance)
  - [Api using DataSets and DataFrames](#api-using-datasets-and-dataframes)
    - [Creating streaming DataFrames and streaming Datasets](#creating-streaming-dataframes-and-streaming-datasets)
    - [Operations on streaming DataFrames/Datasets](#operations-on-streaming-dataframesdatasets)
      - [Basic Operations](#basic-operations)
      - [Window Operations](#window-operations)
        - [Handling Late Data and Watermarking](#handling-late-data-and-watermarking)
    - [Starting Streaming Queries](#starting-streaming-queries)
    - [Managing Streaming Queries](#managing-streaming-queries)
    - [Monitoring Streaming Queries](#monitoring-streaming-queries)
    - [Recovering from Failures with Checkpointing](#recovering-from-failures-with-checkpointing)
    - [Recovery Semantics after Changes in a Streaming Query](#recovery-semantics-after-changes-in-a-streaming-query)
  - [Continuous Processing](#continuous-processing)
- [MLlib](#mllib)
- [GraphX](#graphx)
- [ä½œä¸šæäº¤æ–¹å¼](#ä½œä¸šæäº¤æ–¹å¼)

## åŸºæœ¬æ¦‚å¿µ

- RDD: Resilient Distributed Dataset, å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ã€‚
- Operation: ä½œç”¨äº RDD çš„å„ç§æ“ä½œ, åŒ…æ‹¬ Transformation å’Œ Actionã€‚
- Job: ä½œä¸š, ä¸€ä¸ª Job åŒ…å«å¤šä¸ª RDD åŠä½œç”¨äºç›¸åº” RDD çš„å„ç§æ“ä½œã€‚
- Stage: ä¸€ä¸ªä½œä¸šåˆ†ä¸ºå¤šä¸ªé˜¶æ®µã€‚
- Partition: æ•°æ®åˆ†åŒº, ä¸€ä¸ª RDD ä¸­çš„æ•°æ®å¯ä»¥åˆ†æˆå¤šä¸ªä¸åŒçš„åŒºã€‚
- DAG: Directed Acycle Graph, æœ‰å‘æ— ç¯å›¾, ååº” EDD ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
- Narrow Dependency: çª„ä¾èµ–, å­ RDD ä¾èµ–äºçˆ¶ RDD ä¸­å›ºå®šçš„ Data Partitionã€‚
- Wide Dependency: å®½ä¾èµ–, å­ RDD ä¾èµ–äºçˆ¶ RDD ä¸­çš„æ‰€æœ‰ Data Partition éƒ½æœ‰ä¾èµ–ã€‚
- Caching Management: ç¼“å­˜ç®¡ç†, å¯¹ RDD çš„ä¸­é—´è®¡ç®—ç»“æœè¿›è¡Œç¼“å­˜ç®¡ç†, ä»¥åŠ å¿«æ•´ä½“çš„å¤„ç†é€Ÿåº¦ã€‚

### ç»„ä»¶è¯´æ˜

1. Spark SQL
2. Spark Streaming
3. MLlib
4. GraphX

## RDD

[RDD Programming Guide](http://spark.apache.org/docs/latest/rdd-programming-guide.html)

`RDD` æ˜¯ Spark çš„æœ€åŸºæœ¬æŠ½è±¡, æ˜¯å¯¹åˆ†å¸ƒå¼å†…å­˜çš„æŠ½è±¡ä½¿ç”¨, ä»¥æ“ä½œæœ¬åœ°é›†åˆçš„æ–¹å¼æ¥æ“ä½œåˆ†å¸ƒå¼æ•°æ®é›†çš„æŠ½è±¡å®ç°ã€‚

`RDD` ç”¨äºæ”¯æŒåœ¨å¹¶è¡Œè®¡ç®—æ—¶èƒ½å¤Ÿé«˜æ•ˆåœ°åˆ©ç”¨ä¸­é—´ç»“æœ, æ”¯æŒæ›´ç®€å•çš„ç¼–ç¨‹æ¨¡å‹, åŒæ—¶ä¹Ÿå…·æœ‰åƒ MapReduce ç­‰å¹¶è¡Œè®¡ç®—æ¡†æ¶çš„é«˜å®¹é”™æ€§, èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œè°ƒåº¦åŠå¯æ‰©å±•æ€§ã€‚ `RDD` çš„å®¹é”™é€šè¿‡è®°å½• `RDD` è½¬æ¢æ“ä½œçš„ lineage å…³ç³»æ¥è¿›è¡Œ, lineage è®°å½•äº† `RDD` çš„å®¶æ—å…³ç³», å½“å‡ºç°é”™è¯¯çš„æ—¶å€™, ç›´æ¥é€šè¿‡ lineage è¿›è¡Œæ¢å¤ã€‚ `RDD` æœ€åˆæ•°æ®æŒ–æ˜, æœºå™¨å­¦ä¹ åŠå›¾è®¡ç®—, å› æ­¤è¿™äº›åº”ç”¨æ¶‰åŠåˆ°å¤§å®¶çš„è¿­ä»£è®¡ç®—, åŸºäºå†…å­˜èƒ½å¤Ÿæå¤§åœ°æå‡å…¶åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„æ‰§è¡Œæ•ˆç‡; `RDD` ä¸é€‚ç”¨äºè¯¸å¦‚åˆ†å¸ƒå¼çˆ¬è™«ç­‰éœ€è¦é¢‘ç¹æ›´æ–°å…±äº«çŠ¶æ€çš„ä»»åŠ¡ã€‚

`RDD` åœ¨ Spark ä¸­æ˜¯ä¸€ä¸ªåªè¯»çš„ (valç±»å‹), ç»è¿‡åˆ†åŒºçš„è®°å½•é›†åˆã€‚ `RDD` åœ¨ Spark ä¸­åªæœ‰ä¸¤ç§åˆ›å»ºæ–¹å¼:

1. ä»å­˜å‚¨ç³»ç»Ÿä¸­åˆ›å»º
2. ä»å…¶å®ƒ `RDD` ä¸­åˆ›å»ºã€‚

`RDD` æ˜¯ä¸€ä¸ªä¸å¯åˆ†å‰²çš„åˆ†å¸ƒå¼å¯¹è±¡é›†, æ¯ä¸ª `RDD` å¯ä»¥è¢«åˆ†ä¸ºå¤šä¸ªåˆ†åŒº, è¿™äº›åˆ†åŒºè¿è¡Œåœ¨é›†ç¾¤ä¸­çš„ä¸åŒèŠ‚ç‚¹ä¸Šã€‚

## Spark SQL

[Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)

ç‰¹ç‚¹:

1. æ— ç¼çš„å°† SQL è¯­å¥é›†æˆåˆ° Spark åº”ç”¨ç¨‹åºä¸­
2. ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ–¹å¼
3. å…¼å®¹ Hive
4. å¯é‡‡ç”¨ JDBC/ODBC è¿æ¥

### Dataset

`Dataset` æ˜¯ Spark 1.6 ä¸­æ·»åŠ çš„æ–°æ¥å£, å®ƒæä¾›äº†RDDs(å¼ºç±»å‹, ä½¿ç”¨å¼ºå¤§lambdaå‡½æ•°çš„èƒ½åŠ›)å’Œ Spark SQL ä¼˜åŒ–æ‰§è¡Œå¼•æ“çš„ä¼˜ç‚¹ã€‚ å¯ä»¥ä» JVM å¯¹è±¡æ„é€ æ•°æ®é›†, ç„¶åä½¿ç”¨åŠŸèƒ½è½¬æ¢(map, flatmap, filter ç­‰)è¿›è¡Œæ“ä½œã€‚ `Dataset` API åœ¨ Scala å’Œ Javaä¸­å¯ç”¨ã€‚ Python ä¸æ”¯æŒ Dataset API, ç”±äº Python çš„åŠ¨æ€ç‰¹æ€§, Dataset API çš„ä¼˜åŠ¿å·²ç»å¯ç”¨ã€‚

### DataFrame

`DataFrame` æ˜¯ä¸€ç§ä»¥ `RDD` ä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†, ä¸ä¼ ç»Ÿ `RDBMS` çš„è¡¨ç»“æ„ç±»ä¼¼ã€‚ ä¸ä¸€èˆ¬çš„ `RDD` ä¸åŒçš„æ˜¯, `DataFrame` å¸¦æœ‰ schema å…ƒä¿¡æ¯, å³ `DataFrame` æ‰€è¡¨ç¤ºçš„è¡¨æ•°æ®é›†çš„æ¯ä¸€åˆ—éƒ½å¸¦æœ‰åç§°å’Œç±»å‹, å®ƒå¯¹äºæ•°æ®çš„å†…éƒ¨ç»“æ„å…·æœ‰å¾ˆå¼ºçš„æè¿°èƒ½åŠ›ã€‚

ç‰¹ç‚¹:

1. æ”¯æŒå•æœº(KBçº§)åˆ°é›†ç¾¤(PBçº§åˆ«)çš„æ•°æ®å¤„ç†
2. æ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’Œå­˜å‚¨ç³»ç»Ÿ
3. é€šè¿‡ Spark SQL Catalyst ä¼˜åŒ–å™¨å¯ä»¥è¿›è¡Œé«˜æ•ˆçš„ä»£ç ç”Ÿæˆå’Œä¼˜åŒ–
4. èƒ½å¤Ÿæ— ç¼ç»§æ‰¿æ‰€æœ‰å¤§æ•°æ®å¤„ç†å·¥å…·
5. æä¾› Scala, Java, Python, R API

## Spark Streaming

- [Spark Streaming Programming Guide](http://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [Where to Go from Here](http://spark.apache.org/docs/latest/streaming-programming-guide.html#where-to-go-from-here)

Spark Streaming æ˜¯ Spark æ ¸å¿ƒ API çš„æ‰©å±•, æ”¯æŒå¯¹å®æ—¶æ•°æ®æµè¿›è¡Œå¯ä¼¸ç¼©, é«˜ååé‡å’Œå®¹é”™çš„æµå¤„ç†ã€‚ å¯ä»¥ä» Kafka, Flume, Kinesis, TCP ç­‰å¤šä¸ªæ•°æ®æºè·å–æ•°æ®, ä¹Ÿå¯ä»¥ä½¿ç”¨ map, reduce, join, window ç­‰é«˜çº§å‡½æ•°è¡¨ç¤ºçš„å¤æ‚è¯­æ³•è¿›è¡Œå¤„ç†ã€‚ å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®æ¨é€åˆ°æ–‡ä»¶ç³»ç»Ÿ, æ•°æ®åº“å’Œå®æ—¶ DashBoardã€‚ å¯ä»¥å°† Spark çš„æœºå™¨å­¦ä¹ å’Œå›¾å½¢å¤„ç†ç®—æ³•åº”ç”¨äºæ•°æ®æµã€‚

![streaming_arch](imgs/streaming_arch.png)

å·¥ä½œåŸç†å¦‚ä¸‹: Spark Streaming æ¥æ”¶å®æ—¶è¾“å…¥æ•°æ®æµ, å¹¶å°†æ•°æ®åˆ’åˆ†ä¸º batch, ç„¶åç”± Spark engine å¤„ç†è¿™äº›æ•°æ®, ç”Ÿæˆæœ€ç»ˆçš„ batch ç»“æœæµã€‚

![streaming_flow](imgs/streaming_flow.png)

### Discretized Streams(DStream)

`DStream` æ˜¯ Spark æµæä¾›çš„åŸºæœ¬æŠ½è±¡ã€‚ å®ƒè¡¨ç¤ºè¿ç»­çš„æ•°æ®æµ, æ— è®ºæ˜¯ä»æºæ¥æ”¶çš„è¾“å…¥æ•°æ®æµ, è¿˜æ˜¯é€šè¿‡è½¬æ¢è¾“å…¥æµç”Ÿæˆçš„ç»è¿‡å¤„ç†çš„æ•°æ®æµã€‚ åœ¨å†…éƒ¨,  `DStream` ç”±ä¸€ç³»åˆ—è¿ç»­çš„ `RDDs` è¡¨ç¤ºã€‚ DStream ä¸­çš„æ¯ä¸ª RDD éƒ½åŒ…å«æ¥è‡ªæŸä¸ªæ—¶é—´é—´éš”çš„æ•°æ®, å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![DStream](imgs/dstream.png)

åœ¨ `DStream` ä¸Šåº”ç”¨çš„ä»»ä½•æ“ä½œéƒ½è½¬æ¢ä¸ºåº•å±‚ RDDs ä¸Šçš„æ“ä½œã€‚ ä¾‹å¦‚, åœ¨è¡Œ `DStream` ä¸­çš„æ¯ä¸ª `RDD` ä¸Šåº”ç”¨ `flatMap` æ“ä½œæ¥ç”Ÿæˆå•è¯ `DStream` çš„ RDDsã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![DStream_ops](imgs/dstream_ops.png)

### Input DStreams and Receivers

è¾“å…¥çš„ `DStreams` è¡¨ç¤ºä» Streaming æºæ¥æ”¶æ•°æ®æµçš„ DStreamsã€‚ æ¯ä¸ª `DStream` éƒ½ä¸ä¸€ä¸ª `Reciever` å¯¹è±¡å…³è”, è¯¥å¯¹è±¡æ¥æ”¶æ•°æ®å¹¶å°†å…¶å­˜å‚¨åœ¨ Spark çš„å†…å­˜ä¸­è¿›è¡Œå¤„ç†ã€‚

- Spark æä¾›ä¸¤ç§å†…ç½®çš„ Streaming æº
1. Basic source: åœ¨ç›´æ¥ `StreamingContext` API ä¸­å¯ä»¥ç›´æ¥ä½¿ç”¨çš„, å¦‚: æ–‡ä»¶ç³»ç»Ÿ, Socket è¿æ¥ã€‚
2. Advanced source: ä»ç±»ä¼¼ Kafka, Flume, Kinesis ç­‰é€šè¿‡é¢å¤–çš„ç¨‹åºç±»è·ã€‚

å¦‚æœå¸Œæœ›å¹¶è¡Œæ¥æ”¶å¤šä¸ªæ•°æ®æµ, å¯ä»¥åˆ›å»ºå¤šä¸ª è¾“å…¥ `DStream`, è¯¥æ­¥éª¤å°†åˆ›å»ºå¤šä¸ª `Receiver`, è¿™äº›æ¥æ”¶å™¨å°†åŒæ—¶æ¥æ”¶å¤šä¸ªæ•°æ®æµã€‚ éœ€è¦æ³¨æ„çš„æ˜¯: Spark worker/executor æ˜¯ä¸€ä¸ªé•¿æœŸè¿è¡Œçš„ Task, å› æ­¤å ç”¨åˆ†é…ç»™ Spark Streaming åº”ç”¨çš„ä¸€ä¸ªæ ¸å¿ƒã€‚ Spark Streaming åº”ç”¨éœ€è¦åˆ†é…è¶³å¤Ÿçš„å†…æ ¸(æˆ–çº¿ç¨‹)æ¥å¤„ç†æ¥æ”¶åˆ°çš„æ•°æ®, ä¸€è¾¹è¿è¡Œ Receiverã€‚

`DStream` ä¹Ÿå¯ä»¥é€šè¿‡è‡ªå®šä¹‰ Receiver æ¥è·å–, è§: [Custom Reciever Guide](http://spark.apache.org/docs/latest/streaming-custom-receivers.html)ã€‚

#### Queue of RDDs as a Stream

ä¸ºä½¿ç”¨æµ‹è¯•æ•°æ®æµ‹è¯• Spark Streaming åº”ç”¨, è¿˜å¯ä»¥ä½¿ç”¨ `streamingContext.queueStream(queueOfRDDs)`, åŸºäº RDDs é˜Ÿåˆ—åˆ›å»º `DStream`ã€‚ æ¨å…¥é˜Ÿåˆ—çš„æ¯ä¸ª RDD å°†è¢«è§†ä¸º `DStream` ä¸­çš„ä¸€æ‰¹æ•°æ®, å¹¶åƒ Stream ä¸€æ ·å¤„ç†ã€‚ `StreamingContext` æ›´å¤šç»†èŠ‚å‚è€ƒæ¥å£æ–‡æ¡£: [scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext), [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html), [Python](http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext)ã€‚

#### Transformations on DStream

è½¬æ¢æ“ä½œä¸ RDD ç±»ä¼¼ã€‚

Transformation | Meaning
---|---
map(func) | TODO
flatMap(func) | TODO
filter(func) | TODO
repartition(numPartitions) | TODO
union(otherStream) | TODO
count() | TODO
reduce(func) | TODO
countByValue() | TODO
reduceByKey(func, [numTasks]) | TODO
join(otherStream, [numTasks]) | TODO
transform(func) | TODO
updateStateByKey(func) | TODO

#### UpdateStateByKey Operation

`updaeStateByKey` å…è®¸åœ¨ä½¿ç”¨æ–°ä¿¡æ¯ä¸æ–­æ›´æ–°çŠ¶æ€æ—¶ç»´æŠ¤ä»»æ„çŠ¶æ€ã€‚ åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤:

1. Define the state: çŠ¶æ€å¯ä»¥æ˜¯ä»»æ„æ•°æ®ç±»å‹
2. Define the state update function: é€šè¿‡å‡½æ•°åˆ¶å®šå¦‚ä½•ä½¿ç”¨è¾“å…¥ Stream çš„å‰ä¸€ä¸ªçŠ¶æ€å’Œæ–°å€¼æ›´æ–°çŠ¶æ€ã€‚

åœ¨æ¯ä¸ª batch å¤„ç†ä¸­, Spark å°†ä¸ºæ‰€æœ‰ç°æœ‰ key æ›´æ–°çŠ¶æ€, ä¸ç®¡è¿™äº› key æ˜¯å¦æ–°æ•°æ®ã€‚ å¦‚æœ update å‡½æ•°ä¸è¿”å›ä»»ä½•å€¼, `k-v pair` å°†è¢«æ¶ˆé™¤ã€‚

#### Typical Datasource Imtegration

- kafka: [Spark Streaming + Kafka Integration Guide](http://spark.apache.org/docs/latest/streaming-kafka-integration.html)
- Flume: [Spark Streaming + Flume Integration Guide](http://spark.apache.org/docs/latest/streaming-flume-integration.html)
- Kinesis: [Spark Streaming + Kinesis Integration](http://spark.apache.org/docs/latest/streaming-kinesis-integration.html)

### Window Operation

Spark æä¾›çª—å£è®¡ç®—, å…è®¸åœ¨æ»‘åŠ¨çš„æ•°æ®çª—å£ä¸Šåº”ç”¨è½¬æ¢, å¦‚ä¸‹å›¾:

![Streaming DStream Window](imgs/streaming_dstream_window.png)

çª—å£æ»‘è¿‡ `Source DStream` æ—¶, ä½äºè¯¥çª—å£çš„ Source RDD å°±è¢«ç»„åˆèµ·æ¥å¯¹å…¶è¿›è¡Œæ“ä½œ, ä»è€Œç”Ÿæˆçª—å£åŒ– `DStream` çš„ RDDã€‚ä»»ä½•çª—å£éƒ½éœ€è¦æŒ‡å®šä¸¤ä¸ªå‚æ•°: 

- window length: çª—å£é•¿åº¦(size)
- sliding interval: çª—å£æ“ä½œæ‰§è¡Œçš„æ—¶é—´é—´éš”

```scala
// Reduce last 30 seconds of data, every 10 seconds
val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int, b:Int) => (a + b), Seconds(30), Seconds(30))
```

```java
JavaPairDStream<String, Integer> windowedWordCounts = pair.reduceByKeyAndWindow((i1, i2) -> i1 + i2, Durations.seconds(30), Durations.seconds(10));
```

```python
windowedWorkCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambdax, y: x - y, 30, 10)
```

### DataFrame and SQL Operations

- link: [DataFrame and SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)

é€šè¿‡ `SparkContext` åˆ›å»ºä¸€ä¸ª `SparkSession`, è¯¥ `SparkContext` æ˜¯é€šè¿‡ `StreamingContext` åˆ›å»º, å¯ä»¥åœ¨é©±åŠ¨ç¨‹åºå¤±è´¥æ—¶é‡å¯ã€‚ è¿™æ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªå»¶è¿Ÿå®ä¾‹åŒ–çš„ `SparkSession` æ¥å®ç°çš„ã€‚ usecase:

```scala
val words:DStream(String) = ...
words.foreachRDD {rdd =>
  // Get the singleton instance of SparkSession
  val spark = SparkSession.builder.config(rdd.sparkContext.getConf().getOrCreate())
  import spark.implicits._

  // Convert RDD[String] to DataFrame
  val wordsDataFrame = rdd.toDF("word")

  // Create a temporary view
  wordsDataFrame.createOrReplaceTempView("words")

  // Do word Count on DataFrame using SQL and print it
  val wordCountsDataFrame = spark.sql("select word, count(*) as total from words group by word")

  wordCountsDataFrame.show()
}
```

### Check poing

ä¸€ä¸ª Streaming åº”ç”¨ç¨‹åºå¿…é¡»å…¨å¤©å€™è¿è¡Œ, å› æ­¤å¿…é¡»èƒ½å¤Ÿé€‚åº”ä¸åº”ç”¨ç¨‹åºé€»è¾‘æ— å…³çš„æ•…éšœ(å¦‚: ç³»ç»Ÿæ•…éšœ, JVM å´©æºƒç­‰)ã€‚ ä¸ºæ­¤, Spark Streaming éœ€è¦åˆè¶³å¤Ÿçš„ä¿¡æ¯ checkpoint ä»¥ä¾¿èƒ½å¤Ÿä»æ•…éšœä¸­æ¢å¤ã€‚ æœ‰ä¸¤ç§æ•°æ®æ˜¯ checkpoint çš„ã€‚

- Metadata Checkpoint: å°†å®šä¹‰å…­è®¡ç®—çš„ä¿¡æ¯ä¿å­˜åˆ° HDFS ç­‰å®¹é”™å­˜å‚¨ä¸­, ç”¨äºè¿è¡Œ Streaming Application çš„é©±åŠ¨ç¨‹åºçš„èŠ‚ç‚¹æ•…éšœä¸­æ¢å¤ã€‚ å…ƒæ•°æ®åŒ…æ‹¬:
  - Configuration: ç”¨äºåˆ›å»ºåº”ç”¨ç¨‹åºçš„é…ç½®
  - DStream operations: å®šä¹‰ Streaming Application çš„ DStream æ“ä½œé›†
  - Incomplete batches: ä½œä¸šå·²æ’é˜Ÿä½†å°šæœªå®Œæˆçš„ batch
- DataCheckpoint: å°†ç”Ÿæˆçš„ RDD ä¿å­˜åˆ°å¯é çš„å­˜å‚¨ä¸­ã€‚ åœ¨ä¸€äº›è·¨å¤šä¸ª batch ç»„åˆæ•°æ®çš„æœ‰çŠ¶æ€è½¬æ¢ä¸­æ˜¯å¾ˆæœ‰å¿…è¦çš„ã€‚ åœ¨è¿™ç±»è½¬æ¢ä¸­, ç”Ÿæˆçš„ RDDs ä¾èµ–äºå‰å‡ ä¸ª batch çš„ RDDs, è¿™ä¼šå¯¼è‡´ä¾èµ–é“¾çš„é•¿åº¦ä¸æ–­å¢åŠ ã€‚ ä¸ºé¿å…æ¢å¤æ—¶é—´æ— é™åˆ¶å¢åŠ (ä¸ä¾èµ–é“¾æˆæ­£æ¯”), æœ‰çŠ¶æ€è½¬æ¢ä¸­é—´çš„ RDD ä¼šå®šæœŸ checkpoint åˆ°å¯é çš„å­˜å‚¨(å¦‚: HDFS) ä¸­, ä»¥åˆ‡æ–­ä¾èµ–é“¾ã€‚

Metadata Checkpoint ä¸»è¦ç”¨äºé©±åŠ¨ç¨‹åºä»æ•…éšœä¸­æ¢å¤, Data æˆ– RDD Checkpoint ä¸»è¦ä½œç”¨åœ¨æœ‰çŠ¶æ€è½¬æ¢çš„åŸºæœ¬åŠŸèƒ½ä¸­ã€‚

#### When to enable Checkpointing

å…·å¤‡ä»¥ä¸‹ä»»æ„è¦æ±‚çš„ Application åº”å½“å¯ç”¨ Checkpoint:

- Usage of stateful transformations(æœ‰çŠ¶æ€è½¬æ¢æ—¶): å¦‚æœåº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ `updateStateByKey` æˆ– `reduceByKeyAndWindow`(å…·æœ‰é€†å‡½æ•°), è¯¥æƒ…å†µä¸‹å¿…é¡»æä¾› Checkpoint ç›®å½•, å…è®¸å‘¨æœŸæ€§çš„ RDD Checkpoint
- Recovering from failures of the driver running the application(ä»è¿è¡Œåº”ç”¨ç¨‹åºçš„é©±åŠ¨ç¨‹åºæ•…éšœä¸­æ¢å¤): Metadata Checkpoint ç”¨äºä½¿è¿›åº¦ä¿¡æ¯æ¢å¤

#### How to configure Checkpoint

Checkpoint å¯ä»¥é€šè¿‡åœ¨å®¹é”™, å¯é çš„æ–‡ä»¶ç³»ç»Ÿä¸­è®¾ç½®ä¸€ä¸ªç›®å½•æ¥å¯ç”¨, Checkpoint ä¿¡æ¯å°†ä¿å­˜åœ¨è¯¥ç›®å½•ä¸­ã€‚ é€šè¿‡ `streamingContext.checkpoint(checkpointDirectory)` å®Œæˆã€‚ å¦‚æœéœ€è¦ä»æ•…éšœä¸­æ¢å¤, åº”ç”¨ç¨‹åºåº”å…·æœ‰ä»¥ä¸‹è¡Œä¸º:

- ç¬¬ä¸€æ¬¡å¯åŠ¨ç¨‹åºæ—¶, åˆ›å»ºä¸€ä¸ªæ–°çš„ `StreamingContext`, è®¾ç½®æ‰€æœ‰ Stream, è°ƒç”¨ `start()`
- åœ¨ç¨‹åºå¤±è´¥åé‡å¯æ—¶, åº”ä» Checkpoint ç›®å½•ä¸­çš„ Checkpoint æ•°æ®é‡æ–°åˆ›å»º `StreamingContext`ã€‚

```scala
// Function to create and setup a new StreamingContext
def functionToCreateContext(): StreamingContext = {
  val ssc = new StreamingContext(...) // new Context
  val lines = ssc.socketTextStream(...) // create DStream
  ...
  ssc.checkpoint(checkpointDirectory) // set checkpoint directory
  ssc
}

// Get additional setup on context that needs to be done irrespective of whether it is being started or restarted
context. ...

// Start the context
context.start()
context.awaitTermination()
```

```java
JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() {
  @Override
  JavaStreamingContext jssc = new JavaStreamingContext(...) {
    JavaDStream<String> lines = jssc.socketTextStream(...);
    ...
    jssc.checkpoint(checkpointDirectory);
    return jssc;
  }
};

JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);

context. ...

context.start();
context.awaitTermination();
```

```python
def functionToCreateContext():
  sc = SparkContext(...)
  ssc = StreamingContext(...)
  lines = ssc.socketTextStream(...)
  return ssc

context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)

context. ...

context.start()
context.awaitTermination()
```

### Exactly once

### MLlib Operations

### Monitoring

## Structured Streaming

[Structured Streaming Programming Guide](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)

Streaming/batch ç»Ÿä¸€

### Programming Model

#### Basic Concepts

---
![Data stream as an unbounded table](imgs/structured_as_table.png)

---
![Programming Model for Structured Streaming](imgs/structure_model.png)

---
![Model of the Quick Example](imgs/model_quick_example.png)

#### Handling Event-time and late Data

#### Fault Tolerance

### Api using DataSets and DataFrames

#### Creating streaming DataFrames and streaming Datasets

#### Operations on streaming DataFrames/Datasets

##### Basic Operations

##### Window Operations

![Window Operations](imgs/window_operation.png)

###### Handling Late Data and Watermarking

![Late data handling in Windowed Grouped Aggregation](imgs/late_data.png)

---
![Watermarking in Windowed Grouped Aggregation with Update Model](imgs/wotermarking_in_window_with_update_model.png)

---
![Watermarking in Windowed Grouped Aggregation with Append Mode](imgs/watermarking_in_windowed_with_append_mode.png)

#### Starting Streaming Queries

#### Managing Streaming Queries

#### Monitoring Streaming Queries

#### Recovering from Failures with Checkpointing

#### Recovery Semantics after Changes in a Streaming Query

### Continuous Processing

## MLlib

[Machine Learning Library (MLlib) Guide](http://spark.apache.org/docs/latest/ml-guide.html)

## GraphX

[GraphX Programming Guide](http://spark.apache.org/docs/latest/graphx-programming-guide.html)

GraphXæ˜¯Sparkä¸­å›¾å½¢å’Œå›¾å½¢å¹¶è¡Œè®¡ç®—çš„ä¸€ä¸ªæ–°ç»„ä»¶ã€‚ åœ¨é«˜å±‚æ¬¡ä¸Š, GraphXé€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„å›¾å½¢æŠ½è±¡æ¥æ‰©å±• Spark RDD:ä¸€ä¸ªå¸¦æœ‰é™„åŠ åˆ°æ¯ä¸ªé¡¶ç‚¹å’Œè¾¹ç¼˜çš„å±æ€§çš„æœ‰å‘å¤šå›¾ã€‚ ä¸ºäº†æ”¯æŒå›¾å½¢è®¡ç®—, GraphXå…¬å¼€äº†ä¸€ç»„åŸºæœ¬æ“ä½œç¬¦(ä¾‹å¦‚å­å›¾ã€joinVerticeså’ŒaggregateMessages)ä»¥åŠPregel APIçš„ä¼˜åŒ–å˜ä½“ã€‚ æ­¤å¤–, GraphXè¿˜åŒ…æ‹¬è¶Šæ¥è¶Šå¤šçš„å›¾å½¢ç®—æ³•å’Œæ„å»ºå™¨, ä»¥ç®€åŒ–å›¾å½¢åˆ†æä»»åŠ¡ã€‚

## ä½œä¸šæäº¤æ–¹å¼

[Submitting Applications](http://spark.apache.org/docs/latest/submitting-applications.html)

1. æœ¬æœºè°ƒè¯•, è®¾ç½® `master` ä¸º `local` æ¨¡å¼è¿è¡Œ spark ä½œä¸š, ä¸€èˆ¬ç”¨äºè°ƒè¯•, ä¸ç”¨è¿æ¥é›†ç¾¤ã€‚

```java
SparkSession spark = SparkSession
    .builder()
    .appName("${appName}")
    .master("local")
    .getOrCreate();
```

2. é›†ç¾¤è¿è¡Œ, ä¸€èˆ¬æœ¬æœºè°ƒè¯•åä¼šå°†ä½œä¸šæ‰“æˆ jar åŒ… é€šè¿‡ `spark-submit` æäº¤è¿è¡Œ, ç”Ÿäº§ç¯å¢ƒä¸€èˆ¬ä½¿ç”¨è¯¥ç§æ–¹å¼ã€‚

3. æœ¬åœ°ä½œä¸šè¿è¡Œåˆ°è¿œç¨‹é›†ç¾¤ã€‚

```java
SparkSession spark = SparkSession
    .builder()
    .appName("${appName}")
    .master("{spark://${host}:${port}}") // example: spark://172.24.0.3:7077
    .getOrCreate();
```
