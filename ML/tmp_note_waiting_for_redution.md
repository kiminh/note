1. Relu & Sigmoid

   - Advantage:
    1. Sigmoid: ä¸ä¼š blowing up(çˆ†ç‚¸) æ¿€æ´»
    2. Relu: ä¸ä¼šä½¿æ¢¯åº¦æ¶ˆå¤±
    3. Relu: æ¯” Sigmoid æœ‰æ›´å¤šæœ‰æ•ˆçš„è®¡ç®—, å› ä¸ºåªéœ€è¦å¯¹ $(0,x)$ åŒºé—´çš„å€¼è¿›è¡Œæ“ä½œ
    4. Relu: æ”¶æ•›æ€§æ›´å¥½([Krizhevsky et al](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf))
   - Disadvantage:
    1. 
   - source: [What are the advantages of ReLU over sigmoid function in deep neural networks?](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)

2. æ³¨æ„: ç¥ç»ç½‘ç»œä¸ä¸€å®šå§‹ç»ˆæ¯”ç‰¹å¾ç»„åˆå¥½, ä½†ç¡®å®å¯ä»¥æä¾›é€‚ç”¨äºå¾ˆå¤šæƒ…å½¢çš„çµæ´»æ›¿ä»£æ–¹æ¡ˆã€‚

3. some details:
    1. æ¢¯åº¦æ¶ˆå¤±: è¾ƒä½å±‚(æ›´æ¥è¿‘è¾“å…¥) çš„æ¢¯åº¦å¯èƒ½ä¼šå˜å¾—éå¸¸å°ã€‚ åœ¨æ·±åº¦ç½‘ç»œä¸­, è®¡ç®—è¿™äº›æ¢¯åº¦å¯èƒ½è®¾è®¡è®¸å¤šå°é¡¹çš„ä¹˜ç§¯, å½“è¾ƒä½å±‚çš„æ¢¯åº¦é€æ¸æ¶ˆå¤±åˆ° $0$ æ—¶, è¿™äº›å±‚çš„è®­ç»ƒé€Ÿåº¦ä¼šéå¸¸ç¼“æ…¢, ç”šè‡³ä¸å†è®­ç»ƒã€‚ ReLU æ¿€æ´»å‡½æ•°å¯ä»¥æœ‰åŠ©äºé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
    2. æ¢¯åº¦çˆ†ç‚¸: å¦‚æœç½‘ç»œä¸­çš„æƒé‡è¿‡å¤§, åˆ™è¾ƒä½å±‚çš„æ¢¯åº¦ä¼šè®¾è®¡è®¸å¤šå¤§é¡¹çš„ä¹˜ç§¯, è¯¥ç§æƒ…å†µä¸‹, æ¢¯åº¦ä¼šçˆ†ç‚¸(æ¢¯åº¦è¿‡å¤§å¯¼è‡´éš¾ä»¥æ”¶æ•›)ã€‚
    3. ReLU å•å…ƒæ¶ˆå¤±: ä¸€æ—¦ `ReLU` å•å…ƒçš„åŠ æƒå’Œä½äº $0$, `ReLU` ä½†æ„¿å°±å¯èƒ½ä¼šåœæ»(å³è¯¥èŠ‚ç‚¹çš„è¾“å‡ºå¯¹ç½‘ç»œè¾“å‡ºæ²¡æœ‰ä»»ä½•è´¡çŒ®çš„ $0$ æ¿€æ´»)ã€‚ ç”±äºæ¢¯åº¦çš„æ¥æºè¢«åˆ‡æ–­, `ReLU` çš„è¾“å…¥å¯èƒ½æ— æ³•ä½œå‡ºè¶³å¤Ÿçš„æ”¹å˜æ¥ä½¿åŠ æƒå’Œæ¢å¤åˆ° $0$ ä»¥ä¸Š(é™ä½å­¦ä¹ é€Ÿç‡æœ‰åŠ©äºé˜²æ­¢ `ReLU` å•å…ƒä¸¢å¤±)ã€‚
    4. ä¸¢å¼ƒæ­£åˆ™åŒ–: è¢«ç§°ä¸º`ä¸¢å¼ƒ`çš„å¦ä¸€ç§å½¢å¼çš„æ­£åˆ™åŒ–, å¯ç”¨äºç¥ç»ç½‘ç»œã€‚ åœ¨æ¢¯åº¦ä¸‹é™å‘çš„æ¯ä¸€æ­¥ä¸­éšæœºä¸¢å¼ƒä¸€äº›ç½‘ç»œä½†æ„¿, ä¸¢å¼ƒçš„è¶Šå¤š, æ­£åˆ™åŒ–æ•ˆæœè¶Šå¼ºã€‚
        - 0.0: æ— ä¸¢å¼ƒæ­£åˆ™åŒ–ã€‚
        - 1.0: ä¸¢å¼ƒæ‰€æœ‰å†…å®¹ã€‚æ¨¡å‹å­¦ä¹ ä¸åˆ°ä»»ä½•è§„å¾‹ã€‚

4. $Sofomax$ æ–¹ç¨‹å¼:
$${p(y=j|x)}=\frac{e^{(w_j^Tx+b_j)}}{\sum_{k\in{K}}e^{(w_k^Tx + b_k)}}$$
  > æœ¬è´¨ä¸Šæ˜¯å°†ç½—è¾‘å›å½’å…¬å¼å»¶ä¼¸åˆ°å¤šç±»åˆ«ã€‚
  - **å®Œæ•´ Softmax:**
  - **å€™é€‰é‡‡æ ·:** é’ˆå¯¹æ‰€æœ‰æ­£ç±»åˆ«æ ‡ç­¾è®¡ç®—æ¦‚ç‡, ä½†ä»…é’ˆå¯¹é™„ç±»åˆ«æ ‡ç­¾çš„éšæœºæ ·æœ¬è®¡ç®—æ¦‚ç‡ã€‚
  > ç±»åˆ«æ•°é‡è¾ƒå°‘æ˜¯, å®Œæ•´çš„ $Softmax$ ä»£ä»·å¾ˆå°, ç±»åˆ«çš„å¢åŠ ä¼šå¢åŠ å…¶ä»£ä»·ã€‚ å€™é€‰é‡‡æ ·å¯ä»¥æé«˜å¤„ç†å…·æœ‰å¤§é‡ç±»åˆ«é—®é¢˜çš„æ•ˆç‡ã€‚  
  > $Softmax$ å‡è®¾æ¯ä¸ªæ ·æœ¬åªæ˜¯ä¸€ä¸ªç±»åˆ«çš„æˆå‘˜, ä½†ä¸€äº›æ ·æœ¬å¯ä»¥åŒæ—¶æ˜¯å¤šä¸ªç±»åˆ«çš„æˆå‘˜, å¯¹äºæ­¤ç±»æƒ…å†µ:
  > - ä¸èƒ½ä½¿ç”¨ $Softmax$
  > - å¿…é¡»ä¾èµ–å¤šä¸ªç½—è¾‘å›å½’

5. æ³Šæ¾å›å½’(Poisson Regression) & Softmax å›å½’(Softmax Regression)

    1. æ³Šæ¾å›å½’: å‡è®¾å‘é‡ $x$ è¡¨ç¤ºå¼•èµ·æ—¶é—´å‘ç”Ÿçš„å› ç´ , å‘é‡ $\theta$ è¡¨ç¤ºå› ç´ çš„æƒé‡, åˆ™ä½¿ç”¨ $h_\theta(x)=exp(\theta^{\top}x)$ è¡¨ç¤ºæ—¶é—´å‘ç”Ÿçš„æ¬¡æ•°çš„æœŸæœ›ã€‚ $\theta^{\top}x$ æ¯å¢åŠ ä¸€ä¸ªå•ä½, å°†å¯¼è‡´äº‹ä»¶å‘ç”Ÿæ¬¡æ•°çš„æœŸæœ›å€¼ç¿»å€ã€‚æ­¤æ—¶å› å˜é‡ä¸è‡ªå˜é‡è¿‘ä¼¼æ»¡è¶³æ³Šæ¾åˆ†å¸ƒ $y^i=\pi(h_\theta(x^i))$ã€‚  
    ä¼¼ç„¶å‡½æ•°:
    $$L(y|x;\theta)=\prod_{i=1}^m{P(y^i|x^i;\theta)}=\prod_{i=i}^m\frac{e^{h_\parallel(x^{x^i})}h_\theta(x^i)^{y^i}}{y^i!}$$
    å¯¹æ•°ä¼¼ç„¶å‡½æ•°:
    $$\ln{L(y|x;\theta)=\sum_{i=1}^m(-h_\theta(x^i)+y^i\ln{h_\theta(x^i)-\ln{y^i!}})}$$
    å®šä¹‰æŸå¤±å‡½æ•°:
    $$J(\theta)=\sum_{i=1}^m(-h_\theta(x^i)+y^i\ln{h_\theta(x^i)})$$
    è¦ä½¿ä¼¼ç„¶å‡½æ•°æœ€å¤§, éœ€è¦ä¼¼æŸå¤±å‡½æ•°æœ€å°ã€‚ ä½¿ç”¨æŸå¤±å‡½æ•°çš„æå°å€¼ä»£æ›¿æœ€å°å€¼:
    $$\frac{\partial}{\partial{\theta_j}}J(\theta)=-\frac{1}{m}\sum_{i=1}^m(-h_\theta(x^i)x^j+y_i\frac{1}{h_\theta(x^i)}h_\theta(x^i)x^j),j=0,1,\dots,n$$
    ç®€åŒ–å:
    $$\frac{\partial}{\partial\theta}J{\theta}=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^i)-y^i)x_j^i,j=0,1,\dots,n$$
    ä½¿ç”¨æ¢¯åº¦ä¸‹é™å‘è¿­ä»£æ±‚è§£:
    $$\theta_j^{k+1}=\theta_j^k-\alpha\frac{\partial}{\partial\theta_j}J(\theta),j=0,1,\dots,n,\alphaä¸ºå­¦ä¹ ç‡$$
    2. $Softmax$ å›å½’: ä¹‹å‰çš„ç½—è¾‘å›å½’æ¨¡å‹å¯ä»¥è§£å†³äºŒåˆ†ç±»åˆ†ä½“ã€‚ å°†äºŒåˆ†ç±»æ¨å¹¿ä¸º $k$ åˆ†ç±»é—®é¢˜æœ‰: åœ¨æ¥¼ç»§å›å½’ä¸­, å› å˜é‡ $y\in{\left\{ 1,2,\dots,k \right\}}$ åˆ†åˆ«å¯¹åº” $k$ ä¸ªåˆ†ç±»ã€‚ $Softmax$ å›å½’å‡å®šå› å˜é‡æœä» $\Phi_1,\Phi_2,\dots,\Phi_k$ çš„å¤šé¡¹åˆ†å¸ƒ, å³ $y(i)\sim{Mult(\Phi_1,\Phi_2,\dots,\Phi_k)}$ã€‚å…¶ä¸­:
    $$\phi_i=\frac{\exp(\theta_i^\top{x})}{1+\sum_{l=1}^{k-1}\exp()\theta_l^\top{x}},i=1,2,\dots,k-1$$
    å‚æ•° $\Phi_k$ æ˜¯å†—ä½™çš„, åˆ©ç”¨æ¦‚ç‡ä¹‹å’Œç­‰äº $1$ çš„æ¡ä»¶å¾—:
    $$\phi_k=1-\sum_{i=1}^{k-1}\phi_i=\frac{1}{1+\sum_{l=1}{k-1}\exp(\theta_l^\top{x})}$$
    åŒäº‹å®šä¹‰:
    $$h(x)=\begin{bmatrix}
       \phi_1\phi2\dots\phi_{k-1} 
    \end{bmatrix}^\top$$
    $$\theta=\begin{bmatrix}
       \theta_1\theta_2\dots\theta_{k-1} 
    \end{bmatrix}^\top$$
    $$\theta_i=\begin{bmatrix}
        \theta_{i0}\theta_{i1}\dots\theta_{in}
    \end{bmatrix}^\top,i=1,2,\dots,k$$
    $\Phi$ å…·æœ‰å¦‚ä¸‹æ€§è´¨:
    $$\frac{\partial\ln\phi_i}{\partial\theta_{pq}}=(1-\phi_p)x_q,i=p$$
    $$\frac{\partial\ln\phi_i}{\partial\theta_{pq}}=-\phi_px_q,i=1,2,\dots,k$$
    å°¤å…¶æ³¨æ„: ä¸Šè¿°æ€§è´¨å¯¹ $i=k$ ä¾ç„¶æˆç«‹ã€‚
    å‚æ•° $\theta$ çš„æå¤§ä¼¼ç„¶ä¼°è®¡, ä¼¼ç„¶å‡½æ•°:
    $$L(y|x;\theta)=\prod_{t=1}^mP(y^t|x^t;\theta)=\prod_{t=1}^m\prod_{t=1}^k\phi_i^{1\{y^t=i\}}$$
    å…¶ä¸­, $expression$ å®šä¹‰å¦‚ä¸‹: å½“ $expression$ ä¸ºçœŸæ—¶, å‡½æ•°å€¼ä¸º $1$; å¦åˆ™ä¸º $0$ã€‚ è¿›ä¸€æ­¥ç®€åŒ–:
    å¯¹æ•°ä¼¼ç„¶å‡½æ•°:
    $$\ln{L(y|x;\theta)}=\sum_{t=1}^m\sum_{i=1}^k1\{y^y=i\}\ln{\phi_i}$$
    å®šä¹‰æŸå¤±å‡½æ•°:
    $$J(\theta)=-\frac{1}{m}\sum_{t=1}^m\sum_{i=1}^k1\begin{Bmatrix}
        y^t=i
    \end{Bmatrix}\ln{\phi_i}$$
    è¦ä½¿ä¼¼ç„¶å‡½æ•°æœ€å¤§, åªéœ€ä½¿æŸå¤±å‡½æ•°æœ€å°ã€‚ ä½¿ç”¨æå°å€¼ä»£æ›¿æœ€å°å€¼:
    $$\frac{\partial}{\partial\theta_{pq}}J(\theta)=-\frac{1}{m}\sum_{t=1}^m\sum_{i=1}^k1\begin{Bmatrix}
        y^t=i
    \end{Bmatrix}\frac{\partial}{\partial\theta_{pq}}=-\frac{1}{m}\sum_{t=1}^m\frac{\partial\ln{\phi_i}}{\partial\theta{pq}}$$
    $$=-\frac{1}{m}\sum_{t=1}^m(1\begin{Bmatrix}
        y^t=p
    \end{Bmatrix}-\phi_p)x_q^t$$
    $$=\frac{1}{m}\sum_{t=1}^m(\phi_p-1\begin{Bmatrix}
        y^t=p
    \end{Bmatrix}x_q^t)$$
    è¿›ä¸€æ­¥æ•´ç†ä¸ºå‘é‡å¼:
    $$\frac{\partial}{\partial\theta_{:j}}J(\theta)=\frac{1}{m}\sum_{i=1}^m\begin{pmatrix}
        \begin{bmatrix}
            \phi_1\\\vdots\\\phi_{k-1}
        \end{bmatrix}-\begin{bmatrix}
            1\begin{Bmatrix}
                y^i=1
            \end{Bmatrix}\\\vdots\\1\begin{Bmatrix}
                1\begin{Bmatrix}
                y^i=k-1
            \end{Bmatrix}
            \end{Bmatrix}
        \end{bmatrix}
    \end{pmatrix}x_j^i$$
    $$=\frac{1}{m}\sum_{i=1}^m\begin{pmatrix}
        h_\theta(x^i)-
        \begin{bmatrix}
            1\begin{Bmatrix}
                y^i=1
            \end{Bmatrix}\\\vdots\\1\begin{Bmatrix}
                y^i=k-1
            \end{Bmatrix}
        \end{bmatrix}
    \end{pmatrix}x_j^i,j=1,\dots,n$$
    ä½¿ç”¨æ¢¯åº¦ä¸‹é™å‘è¿­ä»£æ±‚è§£:
    $$\theta_{:j}^{k+1}=\theta_{:j}^k-\partial\frac{\partial}{\partial\theta_{:j}}J(\theta),j=0,1,\dots,n$$


6. 
