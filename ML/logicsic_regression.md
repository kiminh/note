# Logistic Regression 罗辑回归

- [Binary Classification 二项分类问题](#binary-classification-二项分类问题)
- [罗辑回归](#罗辑回归)
- [罗辑回归训练模型](#罗辑回归训练模型)
    - [损失函数](#损失函数)
    - [正则化](#正则化)
        - [L2正则化](#l2正则化)

## Binary Classification 二项分类问题

一种分类任务, 可输出两种互斥类别之一。 例如, 对电子邮件进行评估并输出 `垃圾邮件` 或 `非垃圾邮件` 的机器学习模型就是一个二元分类器。 其输出标记为:

$$y=\begin{cases}
    0,   (垃圾邮件); \\
    1,   (非垃圾邮件);
\end{cases}$$

详细查看[Binary classification](https://en.wikipedia.org/wiki/Binary_classification)

## 罗辑回归

用于线性回归的实属正态分布是用均值参数化的, 这个均值的任何值都是有效的。 但在二项分类问题中, 由于均值必须处于 (0, 1) 之间, 一种方式是使用 `logistic sigmoid` 函数将线性函数的输出压缩进区间 (0, 1)。 该值可以解释为概率:

$$p(y=1|x;\theta)=\sigma({\theta^\top}x)$$
$$y=\frac{1}{1+e^{-z}}$$

如果 z 表示使用罗辑回归训练的模型的线性层的输出, 则会生成一个区间 (0, 1) 的值。表示为:
$$y\prime=\frac{1}{1+e^{-z}}$$
- 其中 $y\prime$ 是罗辑回归模型针对特定样本的输出。
- z 是 ${b+w_1x_1+w_2x_2+...+w_nx_n}$
    - $w$ 值是该模型学习的权重和偏差
    - $x$ 值是特定样本的特征值。

$z$ 也称为对数几率, sigmoid 函数的反函数表明 $z$ 可定义为标签 `1` 的概率除以标签 `0` 的概率得出的值的对数:
$$z={\log{\frac{y}{1-y}}}$$

## 罗辑回归训练模型

### 损失函数

罗辑回归的损失函数是**对数函数**, 定义如下:
$$LogLoss=\sum_{(x,y)\in{D}}{-\log{(y\prime)}-(1-y)\log{(1-y\prime)}}$$

其中:
- $(xy)\in{D}$ 是包含很多标签的样本 $(x,y)$ 的数据集。
- $y$ 是有有标签样本中的标签, 且 $y$ 的每个值为 $0$ 或 $1$。
- $y\prime$ 是对特征集 $x$ 的预测值(介于区间 $(0,1)$)

### 正则化

> 有助于防止出现过拟合

如果没有正则化, 罗辑回归的渐进性会不断促使损失在高纬度空间达到 $0$， 因此大多数模型都会使用以下策略来降低模型复杂性:
- $L_2$ 正则化。
- 早停发, 即 限制训练部署或学习速率。

#### L2正则化

$L_2$ 正则化将正则化项定义为所有特征权重的平方和:
$${L_2 regularization term}={\rVert{s}\rVert}_2^2=w_1^2+w_2^2+...+w_n^2$$

接近 $0$ 的权重对模型复杂度几乎没有影响, 而离群值权重可能会产生巨大影响。
